{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "588e2f83-1c48-4c7a-bc0e-070dd9b2be71",
   "metadata": {},
   "source": [
    "## 🪓 Process knowledge graph dumps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a649405-0f81-4815-bfe0-d62bc67e5bd9",
   "metadata": {},
   "source": [
    "We first need to choose version of Wikipedia, Wikidata, and DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e39534bf-e36b-4b38-9fbe-7008899d40ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<kgdata.dbpedia.config.DBpediaDirCfg at 0x7f6b5f6816d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "from kgdata.wikipedia.config import WikipediaDirCfg\n",
    "from kgdata.dbpedia.config import DBpediaDirCfg\n",
    "from kgdata.wikidata.config import WikidataDirCfg\n",
    "\n",
    "WikipediaDirCfg.init(expanduser(\"~/kgdata/wikipedia/20230620\"))\n",
    "WikidataDirCfg.init(expanduser(\"~/kgdata/wikidata/20230619\"))\n",
    "# DBpediaDirCfg.init(expanduser(\"~/kgdata/dbpedia/20230420\"))\n",
    "DBpediaDirCfg.init(expanduser(\"~/kgdata/dbpedia/20221201\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e17bcc-8c9a-45a6-9fea-521c2a0fdd32",
   "metadata": {},
   "source": [
    "### 🗄 Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f544e-c9e4-4692-9384-7cb29f57b904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 18:18:48.606 | INFO     | kgdata.wikidata.datasets.entities:entities:38 - Getting all entity ids in the dump (including properties)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8834d533bad1484fbe8d1e0482eaac7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "splitting:   0%|          | 0.00/82.8G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from kgdata.wikidata.datasets import import_dataset\n",
    "\n",
    "import_dataset(\"entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eb9493-8944-447a-8987-a489769f50a7",
   "metadata": {},
   "source": [
    "### 🗄 Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a913ac5-7a50-4fda-a6e9-1ebaddeb08f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 11:30:30.292 | DEBUG    | kgdata.spark:get_spark_context:90 - Spark Options: []\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/24 11:30:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2023-07-24 11:30:40.616 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 60966083: https://en.wikipedia.org/wiki/Roeselare_(Chamber_of_Representatives_constituency)\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 21;\n",
      "2023-07-24 11:30:40.912 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 46780784: https://en.wikipedia.org/wiki/SC50_bomb\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2;\n",
      "2023-07-24 11:30:41.087 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 47515979: https://en.wikipedia.org/wiki/1892_Currie_Cup\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 100%\n",
      "2023-07-24 11:30:41.469 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 51565554: https://en.wikipedia.org/wiki/Sajna\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2\"\n",
      "2023-07-24 11:30:41.700 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 14800274: https://en.wikipedia.org/wiki/STX8\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2;\n",
      "2023-07-24 11:30:42.037 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 58077515: https://en.wikipedia.org/wiki/SC_Pick_Szeged_in_European_handball\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 3;\n",
      "2023-07-24 11:30:42.080 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 14763916: https://en.wikipedia.org/wiki/SETDB1\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2;\n",
      "2023-07-24 11:30:42.372 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 30933760: https://en.wikipedia.org/wiki/STARD8\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2;\n",
      "2023-07-24 11:30:42.389 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 68471765: https://en.wikipedia.org/wiki/Robinson_Ekspeditionen_2021\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2\"\n",
      "2023-07-24 11:30:42.465 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 1831130: https://en.wikipedia.org/wiki/Saint_Kitts_and_Nevis_Labour_Party\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4fb0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2!\n",
      "2023-07-24 11:30:42.657 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 71347713: https://en.wikipedia.org/wiki/Saudi_Arabia_at_the_2022_World_Athletics_Championships\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2data-sort-value=\"\"\n",
      "2023-07-24 11:30:42.981 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 46759524: https://en.wikipedia.org/wiki/Rugby_union_at_the_1979_South_Pacific_Games\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4fb0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 100%\n",
      "2023-07-24 11:30:43.048 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 70659936: https://en.wikipedia.org/wiki/Rosyth_(ward)\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 6;\n",
      "2023-07-24 11:30:43.141 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 62930161: https://en.wikipedia.org/wiki/Romania_at_the_2020_Winter_Youth_Olympics\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 4data-sort-value=\"\"\n",
      "2023-07-24 11:30:43.468 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 25148787: https://en.wikipedia.org/wiki/SLX4\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4fb0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2;\n",
      "2023-07-24 11:30:43.463 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 15062263: https://en.wikipedia.org/wiki/SNX5\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.464s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 10316 words\n",
      "[12.531s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 20215 words\n",
      "[12.531s][warning][gc,alloc] Executor task launch worker for task 43.0 in stage 0.0 (TID 43): Retried waiting for GCLocker too often allocating 3121 words\n",
      "[12.531s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 48963 words\n",
      "[12.531s][warning][gc,alloc] Executor task launch worker for task 27.0 in stage 0.0 (TID 27): Retried waiting for GCLocker too often allocating 18570 words\n",
      "[12.531s][warning][gc,alloc] Executor task launch worker for task 10.0 in stage 0.0 (TID 10): Retried waiting for GCLocker too often allocating 23725 words\n",
      "[12.580s][warning][gc,alloc] Executor task launch worker for task 1.0 in stage 0.0 (TID 1): Retried waiting for GCLocker too often allocating 2047 words\n",
      "[12.580s][warning][gc,alloc] Executor task launch worker for task 58.0 in stage 0.0 (TID 58): Retried waiting for GCLocker too often allocating 19390 words\n",
      "[12.658s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 55009 words\n",
      "[12.658s][warning][gc,alloc] Executor task launch worker for task 32.0 in stage 0.0 (TID 32): Retried waiting for GCLocker too often allocating 4098 words\n",
      "[12.658s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 19380 words\n",
      "[12.658s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 3642 words\n",
      "[12.658s][warning][gc,alloc] Executor task launch worker for task 2.0 in stage 0.0 (TID 2): Retried waiting for GCLocker too often allocating 4098 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 11:30:43.681 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 56959229: https://en.wikipedia.org/wiki/Saint_Lucia_at_the_2018_Summer_Youth_Olympics\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2;\n",
      "23/07/24 11:30:43 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapCharBuffer.<init>(HeapCharBuffer.java:64)\n",
      "\tat java.base/java.nio.CharBuffer.allocate(CharBuffer.java:363)\n",
      "\tat java.base/java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:799)\n",
      "\tat org.apache.hadoop.io.Text.decode(Text.java:412)\n",
      "\tat org.apache.hadoop.io.Text.decode(Text.java:389)\n",
      "\tat org.apache.hadoop.io.Text.toString(Text.java:280)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$textFile$2(SparkContext.scala:944)\n",
      "\tat org.apache.spark.SparkContext$$Lambda$2257/0x00000008014c65d0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$Lambda$2883/0x0000000801569e28.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.nio.HeapCharBuffer.<init>(HeapCharBuffer.java:64)\n",
      "\tat java.base/java.nio.CharBuffer.allocate(CharBuffer.java:363)\n",
      "\tat java.base/java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:799)\n",
      "\tat org.apache.hadoop.io.Text.decode(Text.java:412)\n",
      "\tat org.apache.hadoop.io.Text.decode(Text.java:389)\n",
      "\tat org.apache.hadoop.io.Text.toString(Text.java:280)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$textFile$2(SparkContext.scala:944)\n",
      "\tat org.apache.spark.SparkContext$$Lambda$2257/0x00000008014c65d0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$Lambda$2883/0x0000000801569e28.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "2023-07-24 11:30:43.758 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 14819390: https://en.wikipedia.org/wiki/SYMPK\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2;\n",
      "2023-07-24 11:30:43.775 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 55685903: https://en.wikipedia.org/wiki/Saint_Vincent_and_the_Grenadines_at_the_2018_Commonwealth_Games\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a51a0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 2data-sort-value=\"\"\n",
      "23/07/24 11:30:43 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:43 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.String.<init>(String.java:4502)\n",
      "\tat java.base/java.lang.String.<init>(String.java:302)\n",
      "\tat java.base/java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:653)\n",
      "\tat java.base/java.nio.CharBuffer.toString(CharBuffer.java:1859)\n",
      "\tat org.apache.hadoop.io.Text.decode(Text.java:412)\n",
      "\tat org.apache.hadoop.io.Text.decode(Text.java:389)\n",
      "\tat org.apache.hadoop.io.Text.toString(Text.java:280)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$textFile$2(SparkContext.scala:944)\n",
      "\tat org.apache.spark.SparkContext$$Lambda$2257/0x00000008014c65d0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$Lambda$2883/0x0000000801569e28.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.base/java.lang.String.<init>(String.java:4502)\n",
      "\tat java.base/java.lang.String.<init>(String.java:302)\n",
      "\tat java.base/java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:653)\n",
      "\tat java.base/java.nio.CharBuffer.toString(CharBuffer.java:1859)\n",
      "\tat org.apache.hadoop.io.Text.decode(Text.java:412)\n",
      "\tat org.apache.hadoop.io.Text.decode(Text.java:389)\n",
      "\tat org.apache.hadoop.io.Text.toString(Text.java:280)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$textFile$2(SparkContext.scala:944)\n",
      "\tat org.apache.spark.SparkContext$$Lambda$2257/0x00000008014c65d0.apply(Unknown Source)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:320)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:734)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:440)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread$$Lambda$2883/0x0000000801569e28.apply(Unknown Source)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2088)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:274)\n",
      "23/07/24 11:30:43 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:43 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:43 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" 23/07/24 11:30:44 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12.741s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 12519 words\n",
      "[12.741s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 11968 words\n",
      "[12.741s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 28664 words\n",
      "[12.741s][warning][gc,alloc] Executor task launch worker for task 25.0 in stage 0.0 (TID 25): Retried waiting for GCLocker too often allocating 34498 words\n",
      "[12.755s][warning][gc,alloc] Executor task launch worker for task 38.0 in stage 0.0 (TID 38): Retried waiting for GCLocker too often allocating 20415 words\n",
      "[12.839s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 2874 words\n",
      "[12.839s][warning][gc,alloc] Executor task launch worker for task 64.0 in stage 0.0 (TID 64): Retried waiting for GCLocker too often allocating 4098 words\n",
      "[12.839s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 24931 words\n",
      "[12.860s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 27839 words\n",
      "[12.860s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 40005 words\n",
      "[12.861s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 29719 words\n",
      "[12.861s][warning][gc,alloc] Executor task launch worker for task 13.0 in stage 0.0 (TID 13): Retried waiting for GCLocker too often allocating 51315 words\n",
      "[12.911s][warning][gc,alloc] Executor task launch worker for task 48.0 in stage 0.0 (TID 48): Retried waiting for GCLocker too often allocating 4643 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 11:30:44 ERROR Utils: Uncaught exception in thread stdout writer for python3\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Exception in thread \"stdout writer for python3\" java.lang.OutOfMemoryError: Java heap space\n",
      "2023-07-24 11:30:44.229 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 46758330: https://en.wikipedia.org/wiki/Rugby_union_at_the_1971_South_Pacific_Games\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: 100%\n",
      "23/07/24 11:30:44 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_articles.py\", line 48, in deser_html_articles\n",
      "    return HTMLArticle.from_dump_dict(orjson.loads(line))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/models/html_article.py\", line 78, in from_dump_dict\n",
      "    wikitext=o[\"article_body\"][\"wikitext\"],\n",
      "             ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'wikitext'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/07/24 11:30:44 ERROR Executor: Exception in task 25.0 in stage 0.0 (TID 25)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker for task 25.0 in stage 0.0 (TID 25),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 25.0 in stage 0.0 (TID 25) (ckg04.isi.edu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "2023-07-24 11:30:44.372 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 55240461: https://en.wikipedia.org/wiki/Ryan_Freeland\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4fb0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: “1”\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.030s][warning][gc,alloc] stdout writer for python3: Retried waiting for GCLocker too often allocating 9645 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/24 11:30:44 ERROR TaskSetManager: Task 25 in stage 0.0 failed 1 times; aborting job\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3) (ckg04.isi.edu executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "    process()\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_articles.py\", line 48, in deser_html_articles\n",
      "    return HTMLArticle.from_dump_dict(orjson.loads(line))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/models/html_article.py\", line 78, in from_dump_dict\n",
      "    wikitext=o[\"article_body\"][\"wikitext\"],\n",
      "             ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
      "KeyError: 'wikitext'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "23/07/24 11:30:44 ERROR Executor: Exception in task 38.0 in stage 0.0 (TID 38)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 38.0 in stage 0.0 (TID 38),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR Executor: Exception in task 2.0 in stage 0.0 (TID 2)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 2.0 in stage 0.0 (TID 2),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Traceback (most recent call last):\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "       ^^^^^^^^^^^^^^^^\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "23/07/24 11:30:44 ERROR Executor: Exception in task 27.0 in stage 0.0 (TID 27)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 27.0 in stage 0.0 (TID 27),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "       ^^^^^^^^^^^^^^^^\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "       ^^^^^^^^^^^^^^^^\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "Traceback (most recent call last):\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 874, in main\n",
      "    if read_int(infile) == SpecialLengths.END_OF_STREAM:\n",
      "       ^^^^^^^^^^^^^^^^\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "23/07/24 11:30:44 ERROR Executor: Exception in task 32.0 in stage 0.0 (TID 32)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 32.0 in stage 0.0 (TID 32),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 21.0 in stage 0.0 (TID 21) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 52.0 in stage 0.0 (TID 52) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 ERROR Executor: Exception in task 58.0 in stage 0.0 (TID 58)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR SparkUncaughtExceptionHandler: [Container in shutdown] Uncaught exception in thread Thread[Executor task launch worker for task 58.0 in stage 0.0 (TID 58),5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "2023-07-24 11:30:44.565 | ERROR    | kgdata.wikipedia.datasets.html_tables:extract_tables:60 - Error while extracting tables from article 62617750: https://en.wikipedia.org/wiki/Rockhampton%E2%80%93Emu_Park_Road\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 218, in <module>\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 74, in worker\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 830, in main\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 822, in process\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "\n",
      "  File \"/nas/home/binhvu/opt/spark-3.4.1-bin-hadoop3/python/lib/pyspark.zip/pyspark/util.py\", line 81, in wrapper\n",
      "\n",
      "> File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 52, in extract_tables\n",
      "    tables = extractor.extract(\n",
      "             │         └ <method 'extract' of 'rsoup.core.TableExtractor' objects>\n",
      "             └ <rsoup.core.TableExtractor object at 0x7f81948a4bd0>\n",
      "\n",
      "RuntimeError: InvalidCellSpanError: controlled\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 50.0 in stage 0.0 (TID 50) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 42.0 in stage 0.0 (TID 42) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 15.0 in stage 0.0 (TID 15) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 11.0 in stage 0.0 (TID 11) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 39.0 in stage 0.0 (TID 39) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 17.0 in stage 0.0 (TID 17) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 46.0 in stage 0.0 (TID 46) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 16.0 in stage 0.0 (TID 16) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 51.0 in stage 0.0 (TID 51) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 53.0 in stage 0.0 (TID 53) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 4.0 in stage 0.0 (TID 4) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 40.0 in stage 0.0 (TID 40) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 7.0 in stage 0.0 (TID 7) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 54.0 in stage 0.0 (TID 54) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 45.0 in stage 0.0 (TID 45) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 56.0 in stage 0.0 (TID 56) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 68.0 in stage 0.0 (TID 68) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 35.0 in stage 0.0 (TID 35) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 ERROR SparkHadoopWriter: Aborting job job_202307241130352423549159352605059_0009.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 25 in stage 0.0 failed 1 times, most recent failure: Lost task 25.0 in stage 0.0 (TID 25) (ckg04.isi.edu executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2316)\n",
      "\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1593)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n",
      "\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1593)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:572)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:571)\n",
      "\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:577)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "23/07/24 11:30:44 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2961/0x00000008014b2b60@7e729cfe rejected from java.util.concurrent.ThreadPoolExecutor@cfcb1ea[Shutting down, pool size = 1, active threads = 1, queued tasks = 0, completed tasks = 23]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2070)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:835)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:808)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/07/24 11:30:44 WARN TaskSetManager: Lost task 26.0 in stage 0.0 (TID 26) (ckg04.isi.edu executor driver): TaskKilled (Stage cancelled)\n",
      "23/07/24 11:30:44 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2961/0x00000008014b2b60@1e5d63b1 rejected from java.util.concurrent.ThreadPoolExecutor@cfcb1ea[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 24]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2070)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:835)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:808)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/07/24 11:30:44 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2961/0x00000008014b2b60@fc11500 rejected from java.util.concurrent.ThreadPoolExecutor@cfcb1ea[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 24]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2070)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:835)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:808)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/07/24 11:30:44 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2961/0x00000008014b2b60@1a13994d rejected from java.util.concurrent.ThreadPoolExecutor@cfcb1ea[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 24]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2070)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:835)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:808)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/07/24 11:30:44 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2961/0x00000008014b2b60@55e1c250 rejected from java.util.concurrent.ThreadPoolExecutor@cfcb1ea[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 24]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2070)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:835)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:808)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/07/24 11:30:44 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2961/0x00000008014b2b60@d399f68 rejected from java.util.concurrent.ThreadPoolExecutor@cfcb1ea[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 24]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2070)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:835)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:808)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/07/24 11:30:44 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2961/0x00000008014b2b60@7911908a rejected from java.util.concurrent.ThreadPoolExecutor@cfcb1ea[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 24]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2070)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:835)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:808)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/07/24 11:30:44 ERROR TaskSchedulerImpl: Exception in statusUpdate\n",
      "java.util.concurrent.RejectedExecutionException: Task org.apache.spark.scheduler.TaskResultGetter$$Lambda$2961/0x00000008014b2b60@6ffaf3e rejected from java.util.concurrent.ThreadPoolExecutor@cfcb1ea[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 24]\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2070)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:833)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1365)\n",
      "\tat org.apache.spark.scheduler.TaskResultGetter.enqueueFailedTask(TaskResultGetter.scala:139)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.liftedTree2$1(TaskSchedulerImpl.scala:835)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.statusUpdate(TaskSchedulerImpl.scala:808)\n",
      "\tat org.apache.spark.scheduler.local.LocalEndpoint$$anonfun$receive$1.applyOrElse(LocalSchedulerBackend.scala:71)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "23/07/24 11:30:44 ERROR Executor: Exception in task 64.0 in stage 0.0 (TID 64): Java heap space\n",
      "23/07/24 11:30:44 ERROR Executor: Exception in task 10.0 in stage 0.0 (TID 10): Java heap space\n",
      "23/07/24 11:30:44 ERROR Executor: Exception in task 13.0 in stage 0.0 (TID 13): Java heap space\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_86088/4070039712.py\", line 3, in <module>\n",
      "    easy_tables()\n",
      "  File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/easy_tables.py\", line 46, in easy_tables\n",
      "    linked_relational_tables()\n",
      "  File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/linked_relational_tables.py\", line 65, in linked_relational_tables\n",
      "    return linked_tables(\"relational_tables\", lang)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/linked_relational_tables.py\", line 31, in linked_tables\n",
      "    table_dataset: Dataset[Table] = getattr(module, table_dataset_name)()\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/relational_tables.py\", line 14, in relational_tables\n",
      "    html_tables()\n",
      "  File \"/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py\", line 23, in html_tables\n",
      "    .saveAsTextFile(\n",
      "     ^^^^^^^^^^^^^^^\n",
      "  File \"/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/pyspark/rdd.py\", line 3404, in saveAsTextFile\n",
      "    keyed._jrdd.map(self.ctx._jvm.BytesToString()).saveAsTextFile(path, compressionCodec)\n",
      "  File \"/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkgdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwikipedia\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01measy_tables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m easy_tables\n\u001b[0;32m----> 3\u001b[0m \u001b[43measy_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/easy_tables.py:46\u001b[0m, in \u001b[0;36measy_tables\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# print(\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m#     (\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#         linked_relational_tables()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m#     )\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     (\n\u001b[0;32m---> 46\u001b[0m         \u001b[43mlinked_relational_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;241m.\u001b[39mget_rdd()\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;241m.\u001b[39mfilter(partial(is_easy_table, tests\u001b[38;5;241m=\u001b[39mtests))\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;241m.\u001b[39mmap(ser_linked_tables)\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;241m.\u001b[39mcoalesce(\u001b[38;5;241m192\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;241m.\u001b[39msaveAsTextFile(\n\u001b[1;32m     52\u001b[0m             \u001b[38;5;28mstr\u001b[39m(cfg\u001b[38;5;241m.\u001b[39measy_tables),\n\u001b[1;32m     53\u001b[0m             compressionCodecClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.hadoop.io.compress.GzipCodec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     54\u001b[0m         )\n\u001b[1;32m     55\u001b[0m     )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(\n\u001b[1;32m     58\u001b[0m     file_pattern\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39measy_tables \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m, deserialize\u001b[38;5;241m=\u001b[39mdeser_linked_tables\n\u001b[1;32m     59\u001b[0m )\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/linked_relational_tables.py:65\u001b[0m, in \u001b[0;36mlinked_relational_tables\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert Wikipedia links in HTML tables to links in Wikidata using sitelinks\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m    lang: the language of Wikidata\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinked_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelational_tables\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/linked_relational_tables.py:31\u001b[0m, in \u001b[0;36mlinked_tables\u001b[0;34m(table_dataset_name, lang)\u001b[0m\n\u001b[1;32m     30\u001b[0m module \u001b[38;5;241m=\u001b[39m import_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkgdata.wikipedia.datasets.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_dataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m table_dataset: Dataset[Table] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_dataset_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m tbl2titles \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     34\u001b[0m     table_dataset\u001b[38;5;241m.\u001b[39mget_rdd()\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;241m.\u001b[39mflatMap(extract_title_to_tables)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupByKey()\n\u001b[1;32m     40\u001b[0m )\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/relational_tables.py:14\u001b[0m, in \u001b[0;36mrelational_tables\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m does_result_dir_exist(cfg\u001b[38;5;241m.\u001b[39mrelational_tables):\n\u001b[1;32m     13\u001b[0m     (\n\u001b[0;32m---> 14\u001b[0m         \u001b[43mhtml_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;241m.\u001b[39mget_rdd()\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;241m.\u001b[39mfilter(is_relational_table)\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;241m.\u001b[39mmap(ser_table)\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;241m.\u001b[39mcoalesce(\u001b[38;5;241m1024\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;241m.\u001b[39msaveAsTextFile(\n\u001b[1;32m     20\u001b[0m             \u001b[38;5;28mstr\u001b[39m(cfg\u001b[38;5;241m.\u001b[39mrelational_tables),\n\u001b[1;32m     21\u001b[0m             compressionCodecClass\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.hadoop.io.compress.GzipCodec\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m         )\n\u001b[1;32m     23\u001b[0m     )\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(\n\u001b[1;32m     26\u001b[0m     file_pattern\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mrelational_tables \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.gz\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     deserialize\u001b[38;5;241m=\u001b[39mdeser_table,\n\u001b[1;32m     28\u001b[0m )\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/libraries/kgdata/kgdata/wikipedia/datasets/html_tables.py:23\u001b[0m, in \u001b[0;36mhtml_tables\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m does_result_dir_exist(cfg\u001b[38;5;241m.\u001b[39mhtml_tables):\n\u001b[1;32m     18\u001b[0m     (\n\u001b[1;32m     19\u001b[0m         \u001b[43mhtml_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_rdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatMap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_tables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhtml_tables\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompressionCodecClass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morg.apache.hadoop.io.compress.GzipCodec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[1;32m     28\u001b[0m     need_double_check \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/pyspark/rdd.py:3404\u001b[0m, in \u001b[0;36mRDD.saveAsTextFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   3403\u001b[0m     compressionCodec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mjava\u001b[38;5;241m.\u001b[39mlang\u001b[38;5;241m.\u001b[39mClass\u001b[38;5;241m.\u001b[39mforName(compressionCodecClass)\n\u001b[0;32m-> 3404\u001b[0m     \u001b[43mkeyed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBytesToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTextFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompressionCodec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3405\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2116\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2113\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2118\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/ipykernel/zmqshell.py:550\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    544\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    545\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    547\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 550\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    551\u001b[0m }\n\u001b[1;32m    553\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/data/binhvu/gramsplus/.venv/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from kgdata.wikipedia.datasets.easy_tables import easy_tables\n",
    "\n",
    "easy_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0dfe19b-5a31-4809-9014-eeea6236a536",
   "metadata": {},
   "source": [
    "### 🗄 DBpedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab245ec0-9aaa-45a8-b134-d4e617e50bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dbpath = expanduser(\"~/kgdata/databases/dbpedia/20221201\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4f71fc9-673e-4a78-b429-5acb368c92f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 0/8\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1/8\n",
      "\u001b[2K\u001b[1B\u001b[1A"
     ]
    }
   ],
   "source": [
    "!python -m kgdata.dbpedia properties \\\n",
    "    -d {DBpediaDirCfg.get_instance().datadir} \\\n",
    "    -o {dbpath} -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01b2ca7d-8adb-40c2-88af-c1269c47edb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 0/16\n",
      "\u001b[2K\u001b[1B\u001b[1A"
     ]
    }
   ],
   "source": [
    "!python -m kgdata.dbpedia classes \\\n",
    "    -d {DBpediaDirCfg.get_instance().datadir} \\\n",
    "    -o {dbpath} -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb9aa6b-9b51-4f6f-bdd4-f5dd1be01121",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 0/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 17/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 18/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 21/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 68/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 78/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 86/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 87/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 89/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 92/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 94/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 105/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 107/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 108/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 116/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 152/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 162/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 167/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 171/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 173/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 175/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 182/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 189/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 190/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 191/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 192/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 232/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 253/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 255/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 261/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 266/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 271/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 272/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 273/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 332/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 342/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 343/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 345/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 347/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 360/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 362/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 371/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 372/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 416/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 418/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 428/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 432/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 437/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 440/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 445/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 447/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 458/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 459/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 460/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 487/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 510/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 518/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 528/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 530/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 531/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 537/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 546/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 547/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 564/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 600/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 601/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 609/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 618/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 619/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 623/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 630/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 644/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 663/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 677/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 681/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 688/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 693/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 695/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 699/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 702/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 706/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 714/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 727/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 743/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 748/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 760/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 767/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 771/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 775/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 779/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 780/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 786/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 789/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 794/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 802/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 816/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 827/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 840/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 844/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 847/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 856/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 860/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 870/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 877/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 881/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 884/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 894/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 904/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 915/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 924/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 935/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 938/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 948/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 954/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 957/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 971/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 980/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 992/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1001/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1010/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1015/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1021/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1026/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1033/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1038/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1048/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1059/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1069/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1080/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1087/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1093/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1099/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1105/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1119/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1122/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1130/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1143/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1148/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1159/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1165/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1172/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1173/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1175/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1178/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1184/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1193/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1202/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1217/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1235/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1241/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1243/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1246/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1252/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1257/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1263/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1275/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1284/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1293/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1301/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1311/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1315/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1322/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1325/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1333/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░ 1338/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░ 1341/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░ 1354/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░ 1363/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░ 1367/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░ 1382/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░ 1388/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░ 1400/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░ 1404/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░ 1408/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░ 1416/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░ 1423/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░ 1429/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░ 1437/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░ 1444/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░ 1452/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░ 1461/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░ 1470/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░ 1477/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░ 1484/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░ 1490/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░ 1502/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░ 1507/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░ 1513/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░ 1516/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░ 1527/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░ 1538/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░ 1546/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░ 1553/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░ 1560/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░ 1566/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░ 1575/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░ 1584/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░ 1588/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░ 1596/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░ 1602/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░ 1611/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████░░░░░░░░░░░░░░░░░ 1619/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████░░░░░░░░░░░░░░░░░ 1631/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████░░░░░░░░░░░░░░░░░ 1634/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████░░░░░░░░░░░░░░░░░ 1645/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████░░░░░░░░░░░░░░░░ 1653/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████░░░░░░░░░░░░░░░░ 1657/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████░░░░░░░░░░░░░░░░ 1660/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████░░░░░░░░░░░░░░░░ 1663/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████░░░░░░░░░░░░░░░░ 1668/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████░░░░░░░░░░░░░░░ 1678/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████░░░░░░░░░░░░░░░ 1692/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████░░░░░░░░░░░░░░░ 1702/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████░░░░░░░░░░░░░░ 1711/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████░░░░░░░░░░░░░░ 1723/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████░░░░░░░░░░░░░░ 1731/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████░░░░░░░░░░░░░░ 1735/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████░░░░░░░░░░░░░░ 1738/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████░░░░░░░░░░░░░ 1746/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████░░░░░░░░░░░░░ 1756/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████░░░░░░░░░░░░░ 1761/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████░░░░░░░░░░░░░ 1767/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████░░░░░░░░░░░░ 1776/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████░░░░░░░░░░░░ 1785/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████░░░░░░░░░░░░ 1788/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████░░░░░░░░░░░░ 1792/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████░░░░░░░░░░░░ 1801/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████████░░░░░░░░░░░ 1807/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████████░░░░░░░░░░░ 1815/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████████░░░░░░░░░░░ 1828/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████░░░░░░░░░░ 1837/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████░░░░░░░░░░ 1851/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████░░░░░░░░░░ 1860/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████░░░░░░░░░ 1868/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████░░░░░░░░░ 1873/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████░░░░░░░░░ 1874/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████░░░░░░░░░ 1877/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████░░░░░░░░░ 1879/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████░░░░░░░░░ 1886/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████░░░░░░░░░ 1893/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████████░░░░░░░░ 1899/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████████░░░░░░░░ 1902/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████████░░░░░░░░ 1910/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████████░░░░░░░░ 1924/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████████████░░░░░░░ 1936/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████████████░░░░░░░ 1944/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████████████░░░░░░░ 1947/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████████████░░░░░░░ 1957/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████████░░░░░░ 1962/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████████░░░░░░ 1965/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████████░░░░░░ 1971/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████████░░░░░░ 1977/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████████░░░░░░ 1987/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████████░░░░░ 1998/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████████░░░░░ 2004/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████████░░░░░ 2009/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████████░░░░░ 2018/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████████████░░░░ 2030/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████████████░░░░ 2035/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████████████░░░░ 2039/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████████████░░░░ 2049/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████████████████░░░ 2056/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████████████████░░░ 2064/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████████████████░░░ 2073/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████████████░░ 2083/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████████████░░ 2090/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████████████░░ 2105/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████████████░░ 2110/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████████████░ 2124/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████████████░ 2132/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████████████░ 2138/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████████████░ 2142/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████████████████████████████ 2144/2144\n",
      "\u001b[2K\u001b[1B\u001b[1A"
     ]
    }
   ],
   "source": [
    "!python -m kgdata.dbpedia entities \\\n",
    "    -d {DBpediaDirCfg.get_instance().datadir} \\\n",
    "    -o {dbpath} -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a485f26-a500-47d2-bbb3-353d45bb0378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 0/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 147/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 280/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 406/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 584/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 729/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 877/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1056/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1201/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░░░░░ 1386/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A█████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░░░░░░ 1538/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████░░░░░░░░░░░░░░░░░░░ 1678/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A██████████████████████████████████████████████████████░░░░░░░░░░░░░░░ 1831/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A███████████████████████████████████████████████████████████░░░░░░░░░░ 2001/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████████░░░░░ 2153/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████████████░ 2305/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A████████████████████████████████████████████████████████████████████░ 2310/2312\n",
      "\u001b[2K\u001b[1B\u001b[1A"
     ]
    }
   ],
   "source": [
    "!python -m kgdata.dbpedia redirections \\\n",
    "    -d {DBpediaDirCfg.get_instance().datadir} \\\n",
    "    -o {dbpath} -c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gramsplus",
   "language": "python",
   "name": "gramsplus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
